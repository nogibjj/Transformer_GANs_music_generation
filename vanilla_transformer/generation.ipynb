{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!python preprocessing.py  \"/workspaces/Transformer_GANs_music_generation/Dataset/midis_emopia/train\" /workspaces/Transformer_GANs_music_generation/model_path/processed_data.pt 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the trainer...\n",
      "There are 53200 samples in the data, 42560 training samples and 10640 validation samples\n",
      "\n",
      "Beginning training...\n",
      "Epoch 5 Time taken 110.02 seconds Train Loss 1.6954012518538568 Val Loss 1.6252680719793737\n",
      "Epoch 10 Time taken 108.5 seconds Train Loss 1.526021515157886 Val Loss 1.4410936571098305\n",
      "Epoch 15 Time taken 108.02 seconds Train Loss 1.420850113728889 Val Loss 1.3131681777335502\n",
      "Epoch 20 Time taken 108.93 seconds Train Loss 1.342946802046066 Val Loss 1.2135539119308059\n",
      "Epoch 25 Time taken 109.29 seconds Train Loss 1.2842025376800308 Val Loss 1.1294344262675837\n",
      "Epoch 30 Time taken 109.29 seconds Train Loss 1.2374949345015045 Val Loss 1.066127915640135\n",
      "Epoch 35 Time taken 108.8 seconds Train Loss 1.1994111728847474 Val Loss 1.012221186547666\n",
      "Epoch 40 Time taken 107.14 seconds Train Loss 1.1684899432318552 Val Loss 0.9650959250805257\n",
      "Epoch 45 Time taken 104.13 seconds Train Loss 1.1418848398036527 Val Loss 0.9284897607726019\n",
      "Epoch 50 Time taken 103.77 seconds Train Loss 1.119315005738036 Val Loss 0.8977792241551854\n",
      "Checkpointing...\n",
      "Done\n",
      "Saving...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!python train.py /workspaces/Transformer_GANs_music_generation/model_path/processed_data.pt /workspaces/Transformer_GANs_music_generation/model_path/ckpt_path_transformer.pt /workspaces/Transformer_GANs_music_generation/model_path/save_path_transformer.pt 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] [-bs BATCH_SIZE] [-l] [-w WARMUP_STEPS] [-d D_MODEL]\n",
      "                [-nl NUM_LAYERS] [-nh NUM_HEADS] [-dff D_FEEDFORWARD]\n",
      "                [-mrd MAX_REL_DIST] [-map MAX_ABS_POSITION] [-vs VOCAB_SIZE]\n",
      "                [-nb] [-dr DROPOUT] [-le LAYERNORM_EPS]\n",
      "                datapath ckpt_path save_path epochs\n",
      "\n",
      "Train a Music Transformer on single tensor dataset of preprocessed MIDI files\n",
      "\n",
      "positional arguments:\n",
      "  datapath              path at which preprocessed MIDI files are stored as a\n",
      "                        single tensor after being translated into an event\n",
      "                        vocabulary\n",
      "  ckpt_path             path at which to load / store checkpoints while\n",
      "                        training; KeyboardInterrupt while training to\n",
      "                        checkpoint the model; MUST end in .pt or .pth\n",
      "  save_path             path at which to save the model's state dict and\n",
      "                        hyperparameters after training; model will only be\n",
      "                        saved if the training loop finishes before a\n",
      "                        KeyboardInterrupt; MUST end in .pt or .pth\n",
      "  epochs                number of epochs to train for\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -bs BATCH_SIZE, --batch-size BATCH_SIZE\n",
      "                        number of sequences to batch together to compute a\n",
      "                        single training step while training; default: 32\n",
      "  -l, --load-checkpoint\n",
      "                        flag to load a previously saved checkpoint from which\n",
      "                        to resume training; default: False\n",
      "  -w WARMUP_STEPS, --warmup-steps WARMUP_STEPS\n",
      "                        number of warmup steps for transformer learning rate\n",
      "                        scheduler; if loading from checkpoint, this will be\n",
      "                        overwritten by saved value; default: 4000\n",
      "  -d D_MODEL, --d-model D_MODEL\n",
      "                        music transformer hidden dimension size; if loading\n",
      "                        from checkpoint this will be overwritten by saved\n",
      "                        hparams; default: 128\n",
      "  -nl NUM_LAYERS, --num-layers NUM_LAYERS\n",
      "                        number of transformer decoder layers in the music\n",
      "                        transformer; if loading from checkpoint, this will be\n",
      "                        overwritten by saved hparams; default: 3\n",
      "  -nh NUM_HEADS, --num-heads NUM_HEADS\n",
      "                        number of attention heads over which to compute multi-\n",
      "                        head relative attention in the music transformer; if\n",
      "                        loading from checkpoint, this will be overwritten by\n",
      "                        saved hparams; default: 8\n",
      "  -dff D_FEEDFORWARD, --d-feedforward D_FEEDFORWARD\n",
      "                        hidden dimension size of pointwise FFN layers in the\n",
      "                        music transformer; if loading from checkpoint, this\n",
      "                        will be overwritten by saved hparams; default: 512\n",
      "  -mrd MAX_REL_DIST, --max-rel-dist MAX_REL_DIST\n",
      "                        maximum relative distance between tokens to consider\n",
      "                        in relative attention calculation in the music\n",
      "                        transformer; if loading from checkpoint, this will be\n",
      "                        overwritten by saved hparams; default: 1024\n",
      "  -map MAX_ABS_POSITION, --max-abs-position MAX_ABS_POSITION\n",
      "                        maximum absolute length of an input sequence; set this\n",
      "                        to a very large value to be able to generalize to\n",
      "                        longer sequences than in the dataset; if a sequence\n",
      "                        longer than the passed in value is passed into the\n",
      "                        dataset, max_abs_position is set to that value not the\n",
      "                        passed in; if loading from checkpoint, this will be\n",
      "                        overwritten by saved hparams; default: 20000\n",
      "  -vs VOCAB_SIZE, --vocab-size VOCAB_SIZE\n",
      "                        length of the vocabulary in which the input training\n",
      "                        data has been tokenized. if loading from checkpoint,\n",
      "                        this will be overwritten by saved hparams; default:\n",
      "                        416 (size of Oore et. al MIDI vocabulary)\n",
      "  -nb, --no-bias        flag to not use a bias in the linear layers of the\n",
      "                        music transformer; if loading from checkpoint, this\n",
      "                        will be overwritten by saved hparams; default: False\n",
      "  -dr DROPOUT, --dropout DROPOUT\n",
      "                        dropout rate for training the model; if loading from\n",
      "                        checkpoint, this will be overwritten by saved hparams;\n",
      "                        default: 0.1\n",
      "  -le LAYERNORM_EPS, --layernorm-eps LAYERNORM_EPS\n",
      "                        epsilon in layernorm layers to avoid zero division; if\n",
      "                        loading from checkpoint, this will be overwritten by\n",
      "                        saved hparams; default: 1e-6\n"
     ]
    }
   ],
   "source": [
    "!python train.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: generate.py [-h] [-m MODE] [-k TOP_K] [-t TEMPERATURE] [-tm TEMPO] [-v]\n",
      "                   path_to_model save_path\n",
      "\n",
      "Generate midi audio with a Music Transformer!\n",
      "\n",
      "positional arguments:\n",
      "  path_to_model         string path to a .pt file at which has been saved a\n",
      "                        dictionary containing the model state dict and\n",
      "                        hyperparameters\n",
      "  save_path             path at which to save the generated midi file\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -m MODE, --mode MODE  specify 'categorical' or 'argmax' mode of decode\n",
      "                        sampling\n",
      "  -k TOP_K, --top-k TOP_K\n",
      "                        top k samples to consider while decode sampling;\n",
      "                        default: all\n",
      "  -t TEMPERATURE, --temperature TEMPERATURE\n",
      "                        temperature for decode sampling; lower temperature,\n",
      "                        more sure the sampling, higher temperature, more\n",
      "                        diverse the output; default: 1.0 (categorical sample\n",
      "                        of true model output)\n",
      "  -tm TEMPO, --tempo TEMPO\n",
      "                        approximate tempo of generated sample in BMP\n",
      "  -v, --verbose         verbose output flag\n"
     ]
    }
   ],
   "source": [
    "!python generate.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "!python generate.py /workspaces/Transformer_GANs_music_generation/model_path/save_path.pt /workspaces/Transformer_GANs_music_generation/model_path/midi_output.mid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"midiPlayerDiv3148\"></div>\n",
       "        <link rel=\"stylesheet\" href=\"https://cuthbertLab.github.io/music21j/css/m21.css\">\n",
       "        \n",
       "        <script\n",
       "        src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"\n",
       "        ></script>\n",
       "    \n",
       "        <script>\n",
       "        function midiPlayerDiv3148_play() {\n",
       "            const rq = require.config({\n",
       "                paths: {\n",
       "                    'music21': 'https://cuthbertLab.github.io/music21j/releases/music21.debug',\n",
       "                }\n",
       "            });\n",
       "            rq(['music21'], function(music21) {\n",
       "                mp = new music21.miditools.MidiPlayer();\n",
       "                mp.addPlayer(\"#midiPlayerDiv3148\");\n",
       "                mp.base64Load(\"data:audio/midi;base64,TVRoZAAAAAYAAQACJ2BNVHJrAAAAGgD/UQMH0zUA/1kCAAAA/1gEBAIYCM5g/y8ATVRyawAAAL4A/wMFUGlhbm8AwAAA4ABAAMAAgZ1AkDI4zmCAMgCTWJAyMIGJaJA7NACQSEiTWIA7AACQMigAkDU8AJBIQNUokDo0AJBKTLRAgDIAmiCQSEyNEIBIAKBogDIAhkiQMkS7CIBIAJNYgDUAAIA6AACASgAAgEgAAJA1PACQOjQAkEpMAJBITACQRkC7CIBGAJNYkEpEzmCAMgD8WIA1AACAOgAAgEoAAIBIAKBokEpE4jiASgCnMIBKAM5g/y8A\");\n",
       "            });\n",
       "        }\n",
       "        if (typeof require === 'undefined') {\n",
       "            setTimeout(midiPlayerDiv3148_play, 2000);\n",
       "        } else {\n",
       "            midiPlayerDiv3148_play();\n",
       "        }\n",
       "        </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from music21 import converter, instrument, note, chord, stream\n",
    "file = converter.parse('/workspaces/Transformer_GANs_music_generation/model_path/midi_output.mid') \n",
    "file.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"midiPlayerDiv1195\"></div>\n",
       "        <link rel=\"stylesheet\" href=\"https://cuthbertLab.github.io/music21j/css/m21.css\">\n",
       "        \n",
       "        <script\n",
       "        src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"\n",
       "        ></script>\n",
       "    \n",
       "        <script>\n",
       "        function midiPlayerDiv1195_play() {\n",
       "            const rq = require.config({\n",
       "                paths: {\n",
       "                    'music21': 'https://cuthbertLab.github.io/music21j/releases/music21.debug',\n",
       "                }\n",
       "            });\n",
       "            rq(['music21'], function(music21) {\n",
       "                mp = new music21.miditools.MidiPlayer();\n",
       "                mp.addPlayer(\"#midiPlayerDiv1195\");\n",
       "                mp.base64Load(\"data:audio/midi;base64,TVRoZAAAAAYAAQACJ2BNVHJrAAAAFAD/UQMHoSAA/1gEBAIYCM5g/y8ATVRyawAAAeEA/wMFUGlhbm8AwAAA4ABAAMAAzmCQQEgAkDtcAJAyQJNYgEAAAIA7AACAMgCTWJA8RACQO0QAkC4wk1iQK0AAkC0ck1iAKwAAkD1IAJAwRACQQ1STWIA8AACAPQAAkEFUjRCALQCGSJA3WACQS0STWIA7AACQR0wAkENYhkiANwCNEIBBAACQPTgAkE9ck1iALgAAkB8wAJAhVACQPlwAkC4UhkiAPQAAgE8AjRCASwAAgEcAAIAfAACQNTgAkDo0AJBBMACQNlwAkFEgjRCAMACGSJAjEACQK2QAkDBcAJA2XACQSDQAkDBcAJBALACQOGCGSIA1AACAOgCNEIBDAACAIQAAgD4AAIAuAACQMEAAkEFQAJA7OIZIgCMAAIArAACAMAAAgEgAAIAwAI0QgEEAAIA2AACAQQCTWJAzXACQPFyTWIAwAACAMwCTWIBDAACAUQAAgDYAAIBAAACAOAAAgDsAAJBJOACQNEwAkC48AJA3LACQUSAAkDxcAJA9OACQOzgAkCpYAJA3XACQQCwAkDhghkiASQAAgDQAAIAuAACANwCgaIA8AJNYgEAAk1iAPQCnMIAqAACANwCnMIA8APYQgFEAk1iAOACTWJA7OKcwgDsAAIA7AM5g/y8A\");\n",
       "            });\n",
       "        }\n",
       "        if (typeof require === 'undefined') {\n",
       "            setTimeout(midiPlayerDiv1195_play, 2000);\n",
       "        } else {\n",
       "            midiPlayerDiv1195_play();\n",
       "        }\n",
       "        </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from music21 import converter, instrument, note, chord, stream\n",
    "file = converter.parse('/workspaces/Transformer_GANs_music_generation/transformer1 (1).mid') \n",
    "file.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div id=\"midiPlayerDiv3759\"></div>\n",
       "        <link rel=\"stylesheet\" href=\"https://cuthbertLab.github.io/music21j/css/m21.css\">\n",
       "        \n",
       "        <script\n",
       "        src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"\n",
       "        ></script>\n",
       "    \n",
       "        <script>\n",
       "        function midiPlayerDiv3759_play() {\n",
       "            const rq = require.config({\n",
       "                paths: {\n",
       "                    'music21': 'https://cuthbertLab.github.io/music21j/releases/music21.debug',\n",
       "                }\n",
       "            });\n",
       "            rq(['music21'], function(music21) {\n",
       "                mp = new music21.miditools.MidiPlayer();\n",
       "                mp.addPlayer(\"#midiPlayerDiv3759\");\n",
       "                mp.base64Load(\"data:audio/midi;base64,TVRoZAAAAAYAAQACJ2BNVHJrAAAAFAD/UQMHoSAA/1gEBAIYCM5g/y8ATVRyawAAAg8A/wMFUGlhbm8AwAAA4ABAAMAAgs5YkEpQpzCQSlSNEIBKAJoggEoAk1iQKzQAkEdEAJBKUKcwkDIck1iQOzy7CJA8NJNYkEhIAJBMTKcwgDIAAIA7AACAPAAAkC0wAJA3OIZIgCsAAIBHAACASgDIGJA0NJNYgEgAAIBMAACALQAAgDcAAJBDRACQSFQAkDw4AJAtMACQNzgAkExMpzCAQwAAkEM4AJBHUKcwgEgAAIA8AACATAAAkEVIAJApMACQQzyGSIAtAACANwCGSIBDAACARwCGSIA0AKcwkDAsgbEYgEUAAIApAACAQwAAkEVIAJApMACQQzwAkDdAAJA5OACQPDT2EJBFOJNYgEUAAIApAACANwAAkFFQAJAtQACQSESGSIA8AI0QgEMAk1iQNDCTWIA5AJNYgFEAAJBHVACQU1QAkDlEhkiARQCNEJBRYACQRVSTWJA8TKcwgEcAAIBTAJNYgC0AAIBIAACANAAAgDkAAIBRAACARQAAkDBAAJBHUACQU1AAkC1AAJBIRACQPEyGSIBHAACAUwCNEIAtAACASAAAgDwAAJBFVACQUVyTWIA8AJNYkDdAk1iQQ0gAkDxEAJBPWJNYgEMAAJBMUACQQ0STWJBAOJNYgDAAk1iATACTWIBAAIZIgDAAjRCANwAAgDwAAIBPAACAQwCGSIBFAACAUQDOYP8vAA==\");\n",
       "            });\n",
       "        }\n",
       "        if (typeof require === 'undefined') {\n",
       "            setTimeout(midiPlayerDiv3759_play, 2000);\n",
       "        } else {\n",
       "            midiPlayerDiv3759_play();\n",
       "        }\n",
       "        </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from music21 import converter, instrument, note, chord, stream\n",
    "file = converter.parse('/workspaces/Transformer_GANs_music_generation/transformer1 (3).mid') \n",
    "file.show('midi')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
