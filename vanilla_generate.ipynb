{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"d_model\": 128,\n",
    "    \"num_layers\": 3,\n",
    "    \"num_heads\": 8,\n",
    "    \"d_ff\": 512,\n",
    "    \"max_rel_dist\": 1024,\n",
    "    \"max_abs_position\": 0,\n",
    "    \"vocab_size\": 471,\n",
    "    \"bias\": True,\n",
    "    \"dropout\": 0.1,\n",
    "    \"layernorm_eps\": 1e-6\n",
    "}\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "def abs_positional_encoding(max_position, d_model, n=3):\n",
    "    \"\"\"\n",
    "    Since the transformer does not use recurrence or convolution, we have to deliberately give it positional\n",
    "    information. Though learned relative position embeddings will be added to the model, it is possible that absolute\n",
    "    position encoding will aid it in predicting next tokens.\n",
    "\n",
    "    Args:\n",
    "        max_position (int): maximum position for which to calculate positional encoding\n",
    "        d_model (int): Transformer hidden dimension size\n",
    "        n (int): number of dimensions to which to broadcast output\n",
    "\n",
    "    Returns:\n",
    "        sinusoidal absolute positional encoding of shape d_model for max_position positions\n",
    "    \"\"\"\n",
    "    # set of all positions to consider\n",
    "    positions = torch.arange(max_position).float().to(device)\n",
    "\n",
    "    # get angles to input to sinusoid functions\n",
    "    k = torch.arange(d_model).float().to(device)\n",
    "    coeffs = 1 / torch.pow(10000, 2 * (k // 2) / d_model)\n",
    "    angles = positions.view(-1, 1) @ coeffs.view(1, -1)\n",
    "\n",
    "    # apply sin to the even indices of angles along the last axis\n",
    "    angles[:, 0::2] = torch.sin(angles[:, 0::2])\n",
    "\n",
    "    # apply cos to the odd indices of angles along the last axis\n",
    "    angles[:, 1::2] = torch.cos(angles[:, 1::2])\n",
    "\n",
    "    return angles.view(*[1 for _ in range(n-2)], max_position, d_model)\n",
    "\n",
    "\n",
    "def skew(t):\n",
    "    \"\"\"\n",
    "    Implements Huang et. al, 2018's skewing algorithm to correctly reorder the dot(Q, RelativePositionEmbeddings)\n",
    "    matrix. This function generalizes to any shape and any number of dimensions. However, attention calculation\n",
    "    requires shape (..., L, L).\n",
    "\n",
    "    Algorithm:\n",
    "        1. Pad T\n",
    "        2. Reshape\n",
    "        3. Slice\n",
    "\n",
    "    Args:\n",
    "        t (torch.Tensor): tensor to skew\n",
    "\n",
    "    Returns:\n",
    "        Srel: skewed t: nth column from the right is skewed into the nth diagonal under the main; same shape as t\n",
    "    \"\"\"\n",
    "    # pad T\n",
    "    padded = F.pad(t, [1, 0])\n",
    "\n",
    "    # reshape to diagonalize the columns in the last 2 dimensions\n",
    "    Srel = padded.reshape(-1, t.shape[-1] + 1, t.shape[-2])\n",
    "\n",
    "    # final touches\n",
    "    Srel = Srel[:, 1:]              # slice last L values\n",
    "    Srel = Srel.reshape(*t.shape)   # reshape to shape of t\n",
    "    return Srel\n",
    "\n",
    "\n",
    "def rel_scaled_dot_prod_attention(q, k, v, e=None, mask=None):\n",
    "    \"\"\"\n",
    "    A modification given by Shaw et. al, 2018, improved by Huang et. al, 2018, to the Scaled Dot-Product Attention\n",
    "    mechanism given in Vaswani et. al, 2017, which allows the Transformer model to attend to all relevant elements of\n",
    "    the input sequences as well as the relative distances between them.\n",
    "\n",
    "    RelAttention = softmax( mask( QKT + skew(QET) ) / sqrt(d_k) ) V\n",
    "\n",
    "    Args:\n",
    "        q: Queries tensor of shape (..., seq_len_q, d_model)\n",
    "        k: Keys tensor of shape (..., seq_len_k, d_model)\n",
    "        v: Values tensor of shape (..., seq_len_k, d_model)\n",
    "        e (optional): Relative Position Embeddings tensor of shape (seq_len_k, d_model)\n",
    "        mask (optional): mask for input batch with ones indicating the positions to mask\n",
    "\n",
    "    Returns:\n",
    "        output attention of shape (..., seq_len_q, d_model)\n",
    "    \"\"\"\n",
    "    QKt = torch.matmul(q, k.transpose(-1, -2))  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    if e is None:\n",
    "        # assumes q.shape[:-2] == k.shape[:-2]\n",
    "        Srel = torch.zeros(*q.shape[:-2], q.shape[-2], k.shape[-2], device=q.device)\n",
    "    else:\n",
    "        Srel = skew(torch.matmul(q, e.transpose(-1, -2)))  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # find and scale attention logits\n",
    "    dk = sqrt(k.shape[-1])\n",
    "    scaled_attention_logits = (QKt + Srel) / dk  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # add scaled mask to 0 out positions to mask in softmax\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # calculate attention by calculating attention weights by softmaxing on the last dimension\n",
    "    # and then multiplying by v\n",
    "    return torch.matmul(F.softmax(scaled_attention_logits, dim=-1), v)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    MultiHead Relative Attention Block. Computes attention for input batch along num_heads \"heads\".\n",
    "    In the process, attention weights are calculated num_heads times, which allows the network to\n",
    "    extract information from the input batch through several different representations simultaneously\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, max_rel_dist, bias=True,  batch_first=False, tgt_is_causal=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Transformer hidden dimension size\n",
    "            num_heads (int): number of heads along which to calculate attention\n",
    "            max_rel_dist (int): maximum relative distance between positions to consider in creating\n",
    "                                relative position embeddings; set to 0 to compute normal attention\n",
    "            bias (bool, optional): if set to False, all Linear layers in the MHA block will not learn\n",
    "                                   an additive bias. Default: True\n",
    "\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.max_rel_dist = max_rel_dist\n",
    "        self.batch_first = False\n",
    "       \n",
    "\n",
    "        if d_model % num_heads != 0:\n",
    "            raise ValueError(\"d_model must be divisible into num_heads heads\")\n",
    "\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "\n",
    "        self.wq = nn.Linear(self.d_model, self.d_model, bias=bias)  # parameter matrix to generate Q from input\n",
    "        self.wk = nn.Linear(self.d_model, self.d_model, bias=bias)  # parameter matrix to generate K from input\n",
    "        self.wv = nn.Linear(self.d_model, self.d_model, bias=bias)  # parameter matrix to generate V from input\n",
    "\n",
    "        self.E = nn.Embedding(self.max_rel_dist, self.d_model)      # relative position embeddings\n",
    "\n",
    "        self.wo = nn.Linear(self.d_model, self.d_model, bias=True)  # final output layer\n",
    "\n",
    "    @staticmethod\n",
    "    def split_heads(x, num_heads, depth=None):\n",
    "        \"\"\"\n",
    "        Helper function to split input x along num_heads heads\n",
    "\n",
    "        Args:\n",
    "            x: input tensor to split into heads; shape: (..., L, d_model); d_model = num_heads * depth\n",
    "            num_heads (int): number of heads along which to calculate attention\n",
    "            depth (int, optional): desired dimensionality at each head\n",
    "\n",
    "        Returns:\n",
    "            input tensor correctly reshaped and transposed to shape (..., num_heads, L, depth)\n",
    "        \"\"\"\n",
    "        # get depth if None\n",
    "        if depth is None:\n",
    "            if x.shape[-1] % num_heads != 0:\n",
    "                raise ValueError(\"d_model must be divisible into num_heads\")\n",
    "            depth = x.shape[-1] // num_heads\n",
    "\n",
    "        # reshape and transpose x\n",
    "        x = x.view(*x.shape[:-1], num_heads, depth)     # (..., L, num_heads, depth)\n",
    "        return x.transpose(-2, -3)                      # (..., num_heads, L, depth)\n",
    "\n",
    "    def get_required_embeddings(self, seq_len, max_len=None):\n",
    "        \"\"\"\n",
    "        Helper function to get required non-positive relative position embeddings to calculate attention on\n",
    "        input of length seq_len. Required relative position embeddings are:\n",
    "            [last embedding from the right] * max(seq_len - max_len, 0) + Embeddings(max(max_len - seq_len, 0), max_len)\n",
    "\n",
    "        Requires self.E (nn.Embedding): relative position embeddings ordered from E_{-max_len + 1} to E_0\n",
    "\n",
    "        Args:\n",
    "            seq_len (int): length of input sequence\n",
    "            max_len (int, optional): maximum relative distance considered in relative attention calculation\n",
    "                                     Default: E.num_embeddings\n",
    "\n",
    "        Returns:\n",
    "            required relative position embeddings from E\n",
    "        \"\"\"\n",
    "        if max_len is None:\n",
    "            max_len = self.E.num_embeddings\n",
    "\n",
    "        # required relative position embeddings\n",
    "        E_dev = self.E.weight.device\n",
    "        first_emb = self.E(torch.arange(0, 1, device=E_dev)).clone()\n",
    "        return torch.cat(\n",
    "            [*[first_emb.clone() for _ in range(max(seq_len - max_len, 0))],\n",
    "             self.E(torch.arange(max(max_len - seq_len, 0), max_len, device=E_dev))],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Computes Multi-Head Attention on input tensors Q, K, V\n",
    "\n",
    "        Args:\n",
    "            q: Queries tensor of shape (..., seq_len_q, d_model)\n",
    "            k: Keys tensor of shape (..., seq_len_k, d_model)\n",
    "            v: Values tensor of shape (..., seq_len_k, d_model)\n",
    "            mask (optional): mask for input batch with ones indicating positions to mask. Default: None\n",
    "\n",
    "        Returns:\n",
    "            multi-head attention of shape (..., seq_len_q, d_model) for input batch\n",
    "        \"\"\"\n",
    "        # get Q, K, V\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        # get required embeddings from E\n",
    "        seq_len_k = k.shape[-2]\n",
    "        e = self.get_required_embeddings(seq_len_k, self.max_rel_dist)  # (seq_len_k, d_model)\n",
    "\n",
    "        # split into heads\n",
    "        q = self.split_heads(q, self.num_heads, self.depth)  # (batch_size, h, seq_len_q, depth)\n",
    "        k = self.split_heads(k, self.num_heads, self.depth)  # (batch_size, h, seq_len_k, depth)\n",
    "        v = self.split_heads(v, self.num_heads, self.depth)  # (batch_size, h, seq_len_k, depth)\n",
    "        e = self.split_heads(e, self.num_heads, self.depth)  # (h, seq_len_k, depth)\n",
    "\n",
    "        # compute MHA\n",
    "        # attention shape: (batch_size, h, seq_len_q, depth); weights shape: (batch_size, h, seq_len_q, seq_len_k)\n",
    "        rel_scaled_attention = rel_scaled_dot_prod_attention(q, k, v, e, mask=mask)\n",
    "\n",
    "        # concatenate heads and pass through final layer\n",
    "        rel_scaled_attention = rel_scaled_attention.transpose(-2, -3)  # (batch_size, seq_len_q, h, depth)\n",
    "        sh = rel_scaled_attention.shape\n",
    "        return self.wo(rel_scaled_attention.reshape(*sh[:-2], self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "\n",
    "class PointwiseFFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully-connected Feedforward layer that follows the MHA block in each Transformer layer, which is simply a 2 layer\n",
    "    Dense network with a ReLU in between\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, bias=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Transformer hidden dimension size\n",
    "            d_ff (int): intermediate dimension of FFN blocks\n",
    "            bias (bool, optional): if set to False, all Linear layers in the FFN block will not learn\n",
    "                                   an additive bias. Default: True\n",
    "        \"\"\"\n",
    "        super(PointwiseFFN, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=bias),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model, bias=bias)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Every TransformerDecoder layer consists of 2 sublayers:\n",
    "        1. Masked Multi-Head Attention\n",
    "        2. Pointwise Feedforward Network\n",
    "    In the original Transformer, each sublayer further employs a residual connection followed by a LayerNorm on the last\n",
    "    dimension. However, here the LayerNormalization will be placed before the residual connnection, as this Pre-LN\n",
    "    architecture does not generally require an explicitly designed learning rate schedule.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, max_rel_dist, bias=True, dropout=0.1, layernorm_eps=1e-6):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Transformer hidden dimension size\n",
    "            num_heads (int): number of heads along which to calculate attention\n",
    "            d_ff (int): intermediate dimension of FFN blocks\n",
    "            max_rel_dist (int): maximum relative distance between positions to consider in creating\n",
    "                                relative position embeddings; set to 0 to compute normal attention\n",
    "            bias (bool, optional): if set to False, all Linear layers in the Decoder will not learn\n",
    "                                   an additive bias. Default: True\n",
    "            dropout (float in [0, 1], optional): dropout rate for training the model\n",
    "            layernorm_eps (very small positive float, optional): epsilon for LayerNormalization\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.max_rel_idst = max_rel_dist\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, max_rel_dist, bias)\n",
    "        \n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads, max_rel_dist, bias)\n",
    "        self.ffn = PointwiseFFN(d_model, d_ff, bias)\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=layernorm_eps)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=layernorm_eps)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory=None, tgt_mask=None,\n",
    "                memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None, tgt_is_causal=False, memory_is_causal = False):\n",
    "        \"\"\"\n",
    "        Forward pass through decoder layer. Designed to be able to use torch's nn.TransformerDecoder as the final model,\n",
    "        which is why memory and all parameters after tgt_mask are present but are unused.\n",
    "\n",
    "        Args:\n",
    "            tgt: input queries tensor from previous layer, named this way to use nn.TransformerDecoder\n",
    "            tgt_mask (optional, must be explicitly specified as a kwarg): tensor of with 1's indicating positions to\n",
    "                                                                          mask. Default: None\n",
    "\n",
    "        Returns:\n",
    "            output after passing through MHA and FFN blocks, along with intermediate layernorms and residual connections\n",
    "        \"\"\"\n",
    "        # multi-head attention block\n",
    "        attn_out = self.layernorm1(tgt)\n",
    "        attn_out = self.mha(attn_out, attn_out, attn_out, mask=tgt_mask)\n",
    "        attn_out = self.dropout1(attn_out)\n",
    "        attn_out = tgt + attn_out\n",
    "\n",
    "        # pointwise ffn block\n",
    "        ffn_out = self.layernorm2(attn_out)\n",
    "        ffn_out = self.ffn(ffn_out)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "        ffn_out = ffn_out + attn_out\n",
    "\n",
    "        return ffn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from math import sqrt\n",
    "\n",
    "class MusicTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Decoder with Relative Attention. Consists of:\n",
    "        1. Input Embedding\n",
    "        2. Absolute Positional Encoding\n",
    "        3. Stack of N DecoderLayers\n",
    "        4. Final Linear Layer\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_model=hparams[\"d_model\"],\n",
    "                 num_layers=hparams[\"num_layers\"],\n",
    "                 num_heads=hparams[\"num_heads\"],\n",
    "                 d_ff=hparams[\"d_ff\"],\n",
    "                 max_rel_dist=hparams[\"max_rel_dist\"],\n",
    "                 max_abs_position=hparams[\"max_abs_position\"],\n",
    "                 vocab_size=hparams[\"vocab_size\"],\n",
    "                 bias=hparams[\"bias\"],\n",
    "                 dropout=hparams[\"dropout\"],\n",
    "                 layernorm_eps=hparams[\"layernorm_eps\"]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Transformer hidden dimension size\n",
    "            num_heads (int): number of heads along which to calculate attention\n",
    "            d_ff (int): intermediate dimension of FFN blocks\n",
    "            max_rel_dist (int): maximum relative distance between positions to consider in creating\n",
    "                                relative position embeddings. Set to 0 to compute normal attention\n",
    "            max_abs_position (int): maximum absolute position for which to create sinusoidal absolute\n",
    "                                    positional encodings. Set to 0 to compute pure relative attention\n",
    "                                    make it greater than the maximum sequence length in the dataset if nonzero\n",
    "            bias (bool, optional): if set to False, all Linear layers in the MusicTransformer will not learn\n",
    "                                   an additive bias. Default: True\n",
    "            dropout (float in [0, 1], optional): dropout rate for training the model. Default: 0.1\n",
    "            layernorm_eps (very small float, optional): epsilon for LayerNormalization. Default: 1e-6\n",
    "        \"\"\"\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.max_rel_dist = max_rel_dist,\n",
    "        self.max_position = max_abs_position\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.input_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = abs_positional_encoding(max_abs_position, d_model)\n",
    "        self.input_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads, d_ff=d_ff, max_rel_dist=max_rel_dist,\n",
    "                         bias=bias, dropout=dropout, layernorm_eps=layernorm_eps),\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(normalized_shape=d_model, eps=layernorm_eps)\n",
    "        )\n",
    "\n",
    "        self.final = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the Music Transformer. Embeds x according to Vaswani et. al, 2017, adds absolute\n",
    "        positional encoding if present, performs dropout, passes through the stack of decoder layers, and\n",
    "        projects into the vocabulary space. DOES NOT SOFTMAX OR SAMPLE OUTPUT; OUTPUTS LOGITS.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input batch of sequences of shape (batch_size, seq_len)\n",
    "            mask (optional): mask for input batch indicating positions in x to mask with 1's. Default: None\n",
    "\n",
    "        Returns:\n",
    "            input batch after above steps of forward pass through MusicTransformer\n",
    "        \"\"\"\n",
    "        # embed x according to Vaswani et. al, 2017\n",
    "        x = self.input_embedding(x)\n",
    "        x *= sqrt(self.d_model)\n",
    "\n",
    "        # add absolute positional encoding if max_position > 0, and assuming max_position >> seq_len_x\n",
    "        if self.max_position > 0:\n",
    "            x += self.positional_encoding[:, :x.shape[-2], :]\n",
    "\n",
    "        # input dropout\n",
    "        x = self.input_dropout(x)\n",
    "\n",
    "        # pass through decoder\n",
    "        x = self.decoder(x, memory=None, tgt_mask=mask)\n",
    "\n",
    "        # final projection to vocabulary space\n",
    "        return self.final(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Dataset/midis_emopia/train/Q1_1Qc15G0ZHIg_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Jn9r0avp0fY_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_POaIGvLsp5M_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_eVMSeElk81Q_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_1vjy9oMFa8c_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_HY9vPoHbgaI_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_ANZf1QXsNrY_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_fey-8bOR95E_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_vv6nrZ2myXw_4.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_6Uf9XBUD3wE_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_9v2WSpn4FCw_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_SQDuF0qxGQw_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Cg2u_Ldjv8g_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_im4Qxn3GQvo_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_9Yb9OEVwups_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_vv6nrZ2myXw_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_im4Qxn3GQvo_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_aYe-2Glruu4_4.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_bQS8eAXAhyE_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_K6OFDxBU370_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_eVMSeElk81Q_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_e838BE_MH6s_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_ANZf1QXsNrY_5.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1__kJtgm1OUNA_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_2Z9SjI131jA_12.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_xrhWli_R98g_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_doM_LWo-74A_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_VXgRfsVd_o0_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_FfwKrQyQ7WU_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_JT1XJnVmABo_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Jn9r0avp0fY_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_KxlqB3j0zys_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_UYh88SRZC24_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_GY3f6ckBVkA_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_gwmVjvR-sVs_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_doM_LWo-74A_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_UJcnNfeCN1c_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_5NW0zDu6IYM_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Ie5koh4qvJc_12.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_9v2WSpn4FCw_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Xsn9zT-05ns_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_jziI_cuN_H8_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Xsn9zT-05ns_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_bIx4DwkZxFY_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Cg2u_Ldjv8g_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_SQDuF0qxGQw_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_K6OFDxBU370_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1__kJtgm1OUNA_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_jziI_cuN_H8_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_sUJ4a74Skk8_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_VXgRfsVd_o0_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_T92R7xjce34_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_94ROf10d1iA_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_vv6nrZ2myXw_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_xrhWli_R98g_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Y5JcZQ0xg4Y_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_dfNdpy8TUzA_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_TNoaOTrTkTQ_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_7yW9c7t8Hq0_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_BGf5nCdzPOc_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_bQS8eAXAhyE_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_mX-xs3OVhTs_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_9v2WSpn4FCw_5.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_QwsQ8ejbMKg_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_UOSlDydo94E_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1__SJQaaRzD-A_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_1vjy9oMFa8c_4.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_s6-SbDSZzEU_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_POaIGvLsp5M_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_WpZ1rWHOCAQ_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_7yW9c7t8Hq0_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_dfNdpy8TUzA_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_94ROf10d1iA_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_PdwxeMPXOCk_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_JT1XJnVmABo_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_4dXC1cC7crw_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_e8NQ2NH0nc8_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1__8v0MFBZoco_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_pNwQ9Tu_bCs_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_6wFJhmhNeeg_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_2Z9SjI131jA_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Jn9r0avp0fY_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Fc1qk52SaKY_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_s5b8viFsVJE_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_zHwJG8Rrx6c_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_RaUSISlnhKw_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_SlsXsqotUis_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_s5b8viFsVJE_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_FfwKrQyQ7WU_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_eVMSeElk81Q_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_cB-Xh8H_7-Q_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_J6X3rVU1H-c_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_60LLKmpgzRM_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_2Z9SjI131jA_9.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_fey-8bOR95E_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_2Z9SjI131jA_13.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_BGf5nCdzPOc_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_2Z9SjI131jA_4.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_uqRLEByE6pU_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_IH2KrGjKXw0_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_1vjy9oMFa8c_5.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1__kJtgm1OUNA_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Ie5koh4qvJc_6.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_RaUSISlnhKw_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_2Z9SjI131jA_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_UOSlDydo94E_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1__BK2o77sTc0_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_jziI_cuN_H8_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_doM_LWo-74A_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_2Z9SjI131jA_10.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_vv6nrZ2myXw_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_GbUV3TXUzeQ_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_8rupdevqfuI_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_k4dxAlI5N-k_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_VXgRfsVd_o0_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_NGE9ynTJABg_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_gwmVjvR-sVs_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_dfNdpy8TUzA_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_uqRLEByE6pU_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_iFgSjUqI7iM_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_NP0lwB-_-og_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_GbUV3TXUzeQ_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_im4Qxn3GQvo_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_HY9vPoHbgaI_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_e838BE_MH6s_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_biROWEwkDQQ_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Wy2-my19YnY_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_r6laJv1-HMg_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_JT1XJnVmABo_5.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_9v2WSpn4FCw_6.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_wnB2cjd6zbE_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_bIx4DwkZxFY_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_JYVXM0qNQAg_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_4ydjOX3pWds_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_zmZHNy9T8Pg_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_3ahg_eQZhxs_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_sZe3IB1OVfw_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_2Z9SjI131jA_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_3N2G21U7guk_6.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_ldCQ6N9G6Mk_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_ANZf1QXsNrY_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_6kRPHamGDSo_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_0vLPYiPN7qY_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_zHwJG8Rrx6c_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_NGE9ynTJABg_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_zmZHNy9T8Pg_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_FfwKrQyQ7WU_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_XC_SiJszQx0_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_2Z9SjI131jA_8.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_aYe-2Glruu4_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_PdwxeMPXOCk_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_DX1IK3z1w10_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_sZe3IB1OVfw_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Y5JcZQ0xg4Y_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_ANZf1QXsNrY_4.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_TNoaOTrTkTQ_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_osxmQReE_2o_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_4dXC1cC7crw_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_94ROf10d1iA_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_zHwJG8Rrx6c_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_doM_LWo-74A_4.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_RaUSISlnhKw_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_yFw_kO7DF-Y_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_doM_LWo-74A_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_SQDuF0qxGQw_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_8rupdevqfuI_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_94ROf10d1iA_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_6kRPHamGDSo_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_1Qc15G0ZHIg_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_JYVXM0qNQAg_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_bIx4DwkZxFY_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_8izVTDgBQPc_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_o5AIp2Yc01M_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_vv6nrZ2myXw_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_e838BE_MH6s_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Wy2-my19YnY_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_0vLPYiPN7qY_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_yFw_kO7DF-Y_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_JT1XJnVmABo_4.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_9v2WSpn4FCw_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_zmZHNy9T8Pg_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_iFgSjUqI7iM_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1__8v0MFBZoco_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_3N2G21U7guk_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_s6-SbDSZzEU_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_aYe-2Glruu4_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_fey-8bOR95E_3.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_e838BE_MH6s_4.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_WpZ1rWHOCAQ_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_fey-8bOR95E_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_J6X3rVU1H-c_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Y5JcZQ0xg4Y_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_pNwQ9Tu_bCs_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_bQS8eAXAhyE_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_UOSlDydo94E_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_2Z9SjI131jA_11.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_HY9vPoHbgaI_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1__SJQaaRzD-A_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1__BK2o77sTc0_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_gSwv8hZGM-s_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_gSwv8hZGM-s_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_GY3f6ckBVkA_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_Fc1qk52SaKY_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_mX-xs3OVhTs_0.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_yFw_kO7DF-Y_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_miqLU2739dk_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_0vLPYiPN7qY_1.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_V3Y9L4UOcpk_2.mid\n",
      "Parsing Dataset/midis_emopia/train/Q1_nOIBJHkqrE8_0.mid\n",
      "471\n",
      "torch.Size([43997, 100, 1])\n",
      "torch.Size([43997])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/Transformer_GANs_music_generation/vanilla_generate.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bturbo-sniffle-6q7g974vg7vcrjrx/workspaces/Transformer_GANs_music_generation/vanilla_generate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=180'>181</a>\u001b[0m     create_midi(prediction_output, \u001b[39m\"\u001b[39m\u001b[39mtest_output\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bturbo-sniffle-6q7g974vg7vcrjrx/workspaces/Transformer_GANs_music_generation/vanilla_generate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=182'>183</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://codespaces%2Bturbo-sniffle-6q7g974vg7vcrjrx/workspaces/Transformer_GANs_music_generation/vanilla_generate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=183'>184</a>\u001b[0m     main()\n",
      "\u001b[1;32m/workspaces/Transformer_GANs_music_generation/vanilla_generate.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bturbo-sniffle-6q7g974vg7vcrjrx/workspaces/Transformer_GANs_music_generation/vanilla_generate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=172'>173</a>\u001b[0m network_output \u001b[39m=\u001b[39m network_output\u001b[39m.\u001b[39mlong()\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bturbo-sniffle-6q7g974vg7vcrjrx/workspaces/Transformer_GANs_music_generation/vanilla_generate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=173'>174</a>\u001b[0m network_input \u001b[39m=\u001b[39m network_input\u001b[39m.\u001b[39mlong()\n\u001b[0;32m--> <a href='vscode-notebook-cell://codespaces%2Bturbo-sniffle-6q7g974vg7vcrjrx/workspaces/Transformer_GANs_music_generation/vanilla_generate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=174'>175</a>\u001b[0m network_output \u001b[39m=\u001b[39m network_output\u001b[39m.\u001b[39;49msqueeze(\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bturbo-sniffle-6q7g974vg7vcrjrx/workspaces/Transformer_GANs_music_generation/vanilla_generate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=175'>176</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://codespaces%2Bturbo-sniffle-6q7g974vg7vcrjrx/workspaces/Transformer_GANs_music_generation/vanilla_generate.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=176'>177</a>\u001b[0m model \u001b[39m=\u001b[39m train_model(model, network_input[\u001b[39m0\u001b[39m], optimizer, criterion, epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from pathlib import Path\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def get_notes(emotion):\n",
    "    \"\"\" Get all the notes and chords from the midi files \n",
    "    emotion = \"Q1\", \"Q2\", \"Q3\", 'Q4' \"\"\"\n",
    "    \n",
    "    notes = []\n",
    "\n",
    "    for file in Path(\"Dataset/midis_emopia/train\").glob(emotion + \"*\"):\n",
    "        midi = converter.parse(file)\n",
    "        print(\"Parsing %s\" % file)\n",
    "        notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "    return notes\n",
    "    \n",
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 100\n",
    "\n",
    "    # Get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    # Create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length - 1, 1):  # Subtract 1 here\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # Reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "\n",
    "    # Normalize input between -1 and 1\n",
    "    network_input = (network_input - float(n_vocab)/2) / (float(n_vocab)/2)\n",
    "\n",
    "    # to torch tensor\n",
    "    network_input = torch.from_numpy(network_input).float().to(device)\n",
    "    network_output = torch.from_numpy(np.array(network_output)).long().to(device)\n",
    "\n",
    "    return (network_input, network_output)\n",
    "\n",
    "  \n",
    "def create_midi(prediction_output, filename):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for item in prediction_output:\n",
    "        pattern = item[0]\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='{}.mid'.format(filename))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=50000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].detach()\n",
    "\n",
    "\n",
    "# Function to train the model\n",
    "def train_model(model, data, optimizer, criterion, epochs=200):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # squeeze the data\n",
    "        output = output.squeeze(0)\n",
    "        data = data.squeeze(0)\n",
    "\n",
    "        loss = criterion(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch: {}/{}.............\".format(epoch, epochs), end=' ')\n",
    "            print(\"Loss: {:.4f}\".format(loss.item()))\n",
    "    return model\n",
    "\n",
    "def generate(model, input_sequence, seq_len=100):\n",
    "    model.eval()\n",
    "    generated = []\n",
    "    input_sequence = torch.from_numpy(input_sequence).float().to(device)\n",
    "    for i in range(seq_len):\n",
    "        output = model(input_sequence)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        generated.append(output)\n",
    "        input_sequence = np.concatenate((input_sequence[0][1:], output[-1]))\n",
    "        input_sequence = np.reshape(input_sequence, (1, 100, 1))\n",
    "        input_sequence = torch.from_numpy(input_sequence).float().to(device)\n",
    "    return generated\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Get all the notes and chords from the midi files\n",
    "    notes = get_notes(\"Q1\")\n",
    "    n_vocab = len(set(notes))\n",
    "    print(n_vocab)\n",
    "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "    print(network_input.shape)\n",
    "    print(network_output.shape)\n",
    "    # Define the model\n",
    "    model = MusicTransformer(\n",
    "        d_model=128,\n",
    "        num_layers=3,\n",
    "        num_heads=8,\n",
    "        d_ff=512,\n",
    "        max_rel_dist=1024,\n",
    "        max_abs_position=0,\n",
    "        vocab_size=n_vocab,\n",
    "        bias=True,\n",
    "        dropout=0.1,\n",
    "        layernorm_eps=1e-6\n",
    "    ).to(device)\n",
    "\n",
    "    # Define the optimizer and the loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # get Long\n",
    "    network_output = network_output.long()\n",
    "    network_input = network_input.long()\n",
    "    network_output = network_output.squeeze(1)\n",
    "    # Train the model\n",
    "    model = train_model(model, network_input[0], optimizer, criterion, epochs=100)\n",
    "\n",
    "    # Generate a new midi file\n",
    "    prediction_output = generate(model, network_input[0])\n",
    "    create_midi(prediction_output, \"test_output\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
